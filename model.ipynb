{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split #pip install sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_trainDep, y_train1), (x_testDep, y_test1) = tf.keras.datasets.mnist.load_data()\n",
    "draw_classNumbers = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepColor(img):\n",
    "    # 0 = blanc et 1 = noir en binary\n",
    "    data = np.array(img, dtype='uint8' )\n",
    "    image = np.zeros((28,28))\n",
    "    for i in range(0,27):\n",
    "        for j in range(0,27):\n",
    "            if (data[i][j] > 220):\n",
    "                image[i][j] = 255\n",
    "            else :\n",
    "                image[i][j] = 0 \n",
    "    #plt.imshow(image,cmap=\"binary\")\n",
    "    return image\n",
    "# faire un keep color pour reduire info inutile (tracÃ© trop epais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "x_train = np.zeros((len(x_trainDep),28,28))\n",
    "x_test = np.zeros((len(x_testDep),28,28))\n",
    "for img in x_trainDep:\n",
    "    x_train[k]=keepColor(img)\n",
    "    k+=1\n",
    "k = 0\n",
    "for img in x_testDep:\n",
    "    x_test[k]=keepColor(img)\n",
    "    k+=1\n",
    "\n",
    "x_train = x_train.reshape((len(x_trainDep),28,28,1))\n",
    "x_test = x_test.reshape((len(x_testDep),28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../Reconnaissance d image/quick_draw_dataset\" #Dossier\n",
    "files = [name for name in os.listdir(dataset_dir) if \".npy\" in name]\n",
    "max_size_per_cl = 1500\n",
    "draw_classDrawings = []\n",
    "\n",
    "# Evalueate the size of the dataset\n",
    "size = 0\n",
    "for name in files:\n",
    "    draws = np.load(os.path.join(dataset_dir, name))\n",
    "    draws = draws[:max_size_per_cl] # Take only 10 000 draw\n",
    "    size += draws.shape[0]\n",
    "\n",
    "images = np.zeros((size, 28, 28, 1))\n",
    "targets = np.zeros((size,))\n",
    "\n",
    "it = 0\n",
    "t = 0\n",
    "for name in files:\n",
    "    # Open each dataset and add the new class\n",
    "    draw_classDrawings.append(name.replace(\"full_numpy_bitmap_\", \"\").replace(\".npy\", \"\"))\n",
    "    draws = np.load(os.path.join(dataset_dir, name))\n",
    "    draws = draws[:max_size_per_cl] # Take only 10 000 draw\n",
    "    # Add images to the buffer\n",
    "    images[it:it+draws.shape[0]] = np.invert(draws.reshape(-1, 28, 28, 1))\n",
    "    targets[it:it+draws.shape[0]] = t\n",
    "    # Iter\n",
    "    it += draws.shape[0]\n",
    "    t += 1\n",
    "\n",
    "images = images.astype(np.float32)\n",
    "# Shuffle dataset\n",
    "indexes = np.arange(size)\n",
    "np.random.shuffle(indexes)\n",
    "images = images[indexes]\n",
    "targets = targets[indexes]\n",
    "\n",
    "x_trainImgDep, x_testImgDep, y_train1Img, y_test1Img = train_test_split(images, targets, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseColor(img):\n",
    "    # 0 = blanc et 1 = noir en binary\n",
    "    data = np.array(img, dtype='uint8' )\n",
    "    image = np.zeros((28,28))\n",
    "    for i in range(0,27):\n",
    "        for j in range(0,27):\n",
    "            if (data[i][j] > 120):\n",
    "                image[i][j] = 0\n",
    "            else :\n",
    "                image[i][j] = 255 \n",
    "    #plt.imshow(image,cmap=\"binary\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "x_trainImg = np.zeros((len(x_trainImgDep),28,28))\n",
    "x_testImg = np.zeros((len(x_testImgDep),28,28))\n",
    "for img in x_trainImgDep:\n",
    "    x_trainImg[k]=inverseColor(img)\n",
    "    k+=1\n",
    "k = 0\n",
    "for img in x_testImgDep:\n",
    "    x_testImg[k]=inverseColor(img)\n",
    "    k+=1\n",
    "\n",
    "x_trainImg = x_trainImg.reshape((len(x_trainImgDep),28,28,1))\n",
    "x_testImg = x_testImg.reshape((len(x_testImgDep),28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a771f2ca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_trainImgDep[102].reshape(28,28),cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a69a69a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_trainImg[102].reshape(28,28),cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = draw_classNumbers + draw_classDrawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour mettre a la suite des chiffres\n",
    "for i in range(len(y_train1Img)):\n",
    "    y_train1Img[i] = y_train1Img[i]+10\n",
    "for i in range(len(y_test1Img)):\n",
    "    y_test1Img[i] = y_test1Img[i]+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(labels[int(y_train1Img[102])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "#for /=255\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_trainImg = x_trainImg.astype('float32')\n",
    "x_testImg = x_testImg.astype('float32')\n",
    "\n",
    "x_train/=255\n",
    "x_test/=255\n",
    "x_trainImg/=255\n",
    "x_testImg/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTrain = len(x_train) + len(x_trainImg)\n",
    "imagesTrain = np.zeros((nbTrain,28,28,1))\n",
    "for i in range(len(x_train)):\n",
    "    imagesTrain[i] = x_train[i]\n",
    "k = 0\n",
    "for i in range(len(x_train),nbTrain):\n",
    "    imagesTrain[i] = x_trainImg[k]\n",
    "    k += 1\n",
    "    \n",
    "nbTest = len(x_test) + len(x_testImg)\n",
    "imagesTest = np.zeros((nbTest,28,28,1))\n",
    "for i in range(len(x_test)):\n",
    "    imagesTest[i] = x_test[i]\n",
    "k = 0\n",
    "for i in range(len(x_test),nbTest):\n",
    "    imagesTest[i] = x_testImg[k]\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetsTrain1 = np.zeros((nbTrain,))\n",
    "for i in range(len(y_train1)):\n",
    "    targetsTrain1[i] = y_train1[i]\n",
    "k = 0\n",
    "for i in range(len(y_train1),nbTrain):\n",
    "    targetsTrain1[i] = y_train1Img[k]\n",
    "    k += 1\n",
    "    \n",
    "targetsTest1 = np.zeros((nbTest,))\n",
    "for i in range(len(y_test1)):\n",
    "    targetsTest1[i] = y_test1[i]\n",
    "k = 0\n",
    "\n",
    "for i in range(len(y_test1),nbTest):\n",
    "    targetsTest1[i] = y_test1Img[k]\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = 22\n",
    "\n",
    "targetsTrain = tf.keras.utils.to_categorical(targetsTrain1, nb_class)\n",
    "targetsTest = tf.keras.utils.to_categorical(targetsTest1, nb_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "    \n",
    "model.add(tf.keras.layers.Conv2D(64, 3, input_shape=(28,28,1), activation=\"relu\"))\n",
    "tf.keras.layers.BatchNormalization()\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, input_shape=(28,28,1), activation=\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "tf.keras.layers.BatchNormalization()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, input_shape=(28,28,1), activation=\"relu\"))\n",
    "tf.keras.layers.BatchNormalization()\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, input_shape=(28,28,1), activation=\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "tf.keras.layers.BatchNormalization()\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "tf.keras.layers.BatchNormalization()\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(nb_class, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalize = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_generalize = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "train_generator = generalize.flow(imagesTrain, targetsTrain, batch_size=64)\n",
    "test_generator = test_generalize.flow(imagesTest, targetsTest, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-7511dddfc892>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1125 steps, validate for 249 steps\n",
      "Epoch 1/22\n",
      "1125/1125 [==============================] - 604s 537ms/step - loss: 0.4135 - accuracy: 0.8764 - val_loss: 0.2714 - val_accuracy: 0.9166\n",
      "Epoch 2/22\n",
      "1125/1125 [==============================] - 646s 574ms/step - loss: 0.1905 - accuracy: 0.9426 - val_loss: 0.2125 - val_accuracy: 0.9389\n",
      "Epoch 3/22\n",
      "1125/1125 [==============================] - 630s 560ms/step - loss: 0.1483 - accuracy: 0.9551 - val_loss: 0.1805 - val_accuracy: 0.9492\n",
      "Epoch 4/22\n",
      "1125/1125 [==============================] - 651s 578ms/step - loss: 0.1281 - accuracy: 0.9614 - val_loss: 0.1745 - val_accuracy: 0.9529\n",
      "Epoch 5/22\n",
      "1125/1125 [==============================] - 651s 579ms/step - loss: 0.1159 - accuracy: 0.9646 - val_loss: 0.1619 - val_accuracy: 0.9561\n",
      "Epoch 6/22\n",
      "1125/1125 [==============================] - 590s 525ms/step - loss: 0.1034 - accuracy: 0.9682 - val_loss: 0.1549 - val_accuracy: 0.9571\n",
      "Epoch 7/22\n",
      "1125/1125 [==============================] - 311s 277ms/step - loss: 0.0942 - accuracy: 0.9715 - val_loss: 0.1751 - val_accuracy: 0.9537\n",
      "Epoch 8/22\n",
      "1125/1125 [==============================] - 288s 256ms/step - loss: 0.0877 - accuracy: 0.9732 - val_loss: 0.1674 - val_accuracy: 0.9565\n",
      "Epoch 9/22\n",
      "1125/1125 [==============================] - 273s 243ms/step - loss: 0.0834 - accuracy: 0.9739 - val_loss: 0.1568 - val_accuracy: 0.9598\n",
      "Epoch 10/22\n",
      "1125/1125 [==============================] - 259s 230ms/step - loss: 0.0743 - accuracy: 0.9772 - val_loss: 0.1470 - val_accuracy: 0.9610\n",
      "Epoch 11/22\n",
      "1125/1125 [==============================] - 259s 231ms/step - loss: 0.0711 - accuracy: 0.9776 - val_loss: 0.1683 - val_accuracy: 0.9578\n",
      "Epoch 12/22\n",
      "1125/1125 [==============================] - 261s 232ms/step - loss: 0.0682 - accuracy: 0.9788 - val_loss: 0.1690 - val_accuracy: 0.9559\n",
      "Epoch 13/22\n",
      "1125/1125 [==============================] - 266s 237ms/step - loss: 0.0642 - accuracy: 0.9799 - val_loss: 0.1507 - val_accuracy: 0.9613\n",
      "Epoch 14/22\n",
      "1125/1125 [==============================] - 270s 240ms/step - loss: 0.0586 - accuracy: 0.9811 - val_loss: 0.1582 - val_accuracy: 0.9613\n",
      "Epoch 15/22\n",
      "1125/1125 [==============================] - 262s 233ms/step - loss: 0.0593 - accuracy: 0.9816 - val_loss: 0.1549 - val_accuracy: 0.9623\n",
      "Epoch 16/22\n",
      "1125/1125 [==============================] - 284s 252ms/step - loss: 0.0555 - accuracy: 0.9825 - val_loss: 0.1647 - val_accuracy: 0.9606\n",
      "Epoch 17/22\n",
      "1125/1125 [==============================] - 282s 250ms/step - loss: 0.0521 - accuracy: 0.9835 - val_loss: 0.1960 - val_accuracy: 0.9567\n",
      "Epoch 18/22\n",
      "1125/1125 [==============================] - 262s 233ms/step - loss: 0.0522 - accuracy: 0.9834 - val_loss: 0.1761 - val_accuracy: 0.9596\n",
      "Epoch 19/22\n",
      "1125/1125 [==============================] - 254s 226ms/step - loss: 0.0482 - accuracy: 0.9849 - val_loss: 0.1768 - val_accuracy: 0.9588\n",
      "Epoch 20/22\n",
      "1125/1125 [==============================] - 261s 232ms/step - loss: 0.0468 - accuracy: 0.9856 - val_loss: 0.1716 - val_accuracy: 0.9590\n",
      "Epoch 21/22\n",
      "1125/1125 [==============================] - 253s 225ms/step - loss: 0.0434 - accuracy: 0.9861 - val_loss: 0.1788 - val_accuracy: 0.9623\n",
      "Epoch 22/22\n",
      "1125/1125 [==============================] - 253s 225ms/step - loss: 0.0461 - accuracy: 0.9852 - val_loss: 0.1705 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4a804f7400>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbTrain = imagesTrain.shape[0]\n",
    "nbTest = imagesTest.shape[0]\n",
    "# nb de batch => 60000//64 (64 img par batch) => 937\n",
    "# une epoch => 937 * 1 batch (parcour toutes les images et apprends 64 par 64)\n",
    "# fait 20 ces epochs, entre le model a appris et les erreurs se reduisent \n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=nbTrain//64, epochs=22, \n",
    "                    validation_data=test_generator, validation_steps=nbTest//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f4a699a5550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for keyword: 1, expecting 2\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f4a699a5550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for keyword: 1, expecting 2\n",
      "WARNING:tensorflow:From /usr/lib/python3.8/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1782: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openImageKeepColor(filename):\n",
    "    img = Image.open( filename + '.jpg' )\n",
    "    data = np.array(img, dtype='uint8' )\n",
    "    image = np.zeros((28,28))\n",
    "    for i in range(0,27):\n",
    "        for j in range(0,27):\n",
    "            image[i][j] = data[i][j][0]\n",
    "    #plt.imshow(image,cmap=\"binary\")\n",
    "    return image\n",
    "\n",
    "def openImageInverseColor(filename):\n",
    "    # 0 = blanc et 1 = noir en binary\n",
    "    img = Image.open(filename + '.jpg')\n",
    "    data = np.array(img, dtype='uint8')\n",
    "    image = np.zeros((28, 28))\n",
    "    for i in range(0, 27):\n",
    "        for j in range(0, 27):\n",
    "            if (data[i][j][0] > 120):\n",
    "                image[i][j] = 0\n",
    "            else:\n",
    "                image[i][j] = 255\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesImages = [\"../Cours Tensorflow/zero\",\"../Cours Tensorflow/un\",\"../Cours Tensorflow/deux\",\"../Cours Tensorflow/trois\",\"../Cours Tensorflow/quatre\",\"../Cours Tensorflow/cinq\",\"../Cours Tensorflow/six\",\"../Cours Tensorflow/sept\",\"../Cours Tensorflow/huit\",\"../Cours Tensorflow/neuf\",\"../Reconnaissance d image/dog\",\"../Reconnaissance d image/chair\",\"../Reconnaissance d image/eye\",\"../Reconnaissance d image/face\",\"monDessin\",\"../Reconnaissance d image/tourEiffel2\",\"../Reconnaissance d image/eye2\",\"../Reconnaissance d image/face2\",\"../Reconnaissance d image/face3\"]\n",
    "images = np.zeros((20,28,28))\n",
    "\n",
    "for i in range(0,10):\n",
    "    image = openImageInverseColor(mesImages[i])\n",
    "    images[i] = image\n",
    "    images[i] /= 255\n",
    "for i in range(10,19):\n",
    "    image = openImageInverseColor(mesImages[i])\n",
    "    images[i] = image\n",
    "    images[i] /= 255\n",
    "images = images.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'airplane', 'apple', 'book', 'brain', 'car', 'chair', 'dog', 'eye', 'face', 'guitar', 'rabbit', 'The Eiffel Tower']\n"
     ]
    }
   ],
   "source": [
    "labelsTest = [0,1,2,3,4,5,6,7,8,9,\"dog\",\"chair\",\"eye\",\"face\",\"rabbit\",\"The Eiffel Tower\",\"eye\",\"face\",\"face\"]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbit dog\n",
      "loss :  5.0 %\n",
      "[10]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(images)\n",
    "loss = list()\n",
    "lossTaux = 0\n",
    "for i in range(19):\n",
    "    if (labels[np.argmax(predictions[i])] != labelsTest[i]):\n",
    "        loss.append(i)\n",
    "        print(labels[np.argmax(predictions[i])],labelsTest[i])\n",
    "        lossTaux+=1\n",
    "print(\"loss : \",(lossTaux/20)*100,\"%\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "monImage = np.zeros((1,28,28))\n",
    "\n",
    "img = openImageInverseColor(\"../Reconnaissance d image/dog\")\n",
    "img /=255\n",
    "img = img.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "rabbit\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(img)\n",
    "print(np.argmax(prediction))\n",
    "print(labels[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a73d24fd0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.reshape(28,28),cmap=\"binary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
